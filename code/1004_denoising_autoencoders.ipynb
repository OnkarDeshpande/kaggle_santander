{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAE:\n",
    "\n",
    "The Denoising Autoencoder, is basically reconstruction of corrupted data.\n",
    "    \n",
    "    \n",
    "Autoencoders are Neural Networks which are commonly used for feature selection and extraction. However, when there are more nodes in the hidden layer than there are inputs, the Network is risking to learn the so-called “Identity Function”, also called “Null Function”, meaning that the output equals the input, marking the Autoencoder useless.\n",
    "\n",
    "Denoising Autoencoders solve this problem by corrupting the data on purpose by randomly turning some of the input values to zero. In general, the percentage of input nodes which are being set to zero is about 50%. Other sources suggest a lower count, such as 30%. It depends on the amount of data and input nodes you have.\n",
    "\n",
    "\n",
    "When calculating the Loss function, it is important to compare the output values with the original input, not with the corrupted input. That way, the risk of learning the identity function instead of extracting features is eliminated.\n",
    "\n",
    "VVIMP\n",
    "https://reyhaneaskari.github.io/AE.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../data/train.csv'\n",
    "\n",
    "df_train = pd.read_csv(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train[df_train.columns[2:]].head()\n",
    "# list(df_train.iloc[5][2:])\n",
    "df_train['target'].iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0],\n",
    "                         std=[1])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, df, transform=None):\n",
    "        'Initialization'\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "\n",
    "        # Load data and get label\n",
    "        X = list(self.df.iloc[index][2:].values)\n",
    "        y = self.df['target'].iloc[index]\n",
    "        \n",
    "        \n",
    "        sample = {'data': X, 'label': y}\n",
    "\n",
    "        \n",
    "        if self.transform:\n",
    "            sample['data'] = self.transform(torch.stack(X))\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 256\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.from_numpy(df_train[df_train.columns[2:]].values))\n",
    "y = Variable(torch.from_numpy(df_train['target'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Dataset(df_train)\n",
    "dataset = TensorDataset(x, y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "#                                         nn.Linear(200, 128),\n",
    "#                                         nn.ReLU(True),\n",
    "                                        nn.Linear(200, 64),\n",
    "                                        nn.ReLU(True),\n",
    "                                        nn.Linear(64, 16),\n",
    "                                        nn.ReLU(True)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "                                        nn.Linear(16, 64),\n",
    "                                        nn.ReLU(True),\n",
    "                                        nn.Linear(64, 200),\n",
    "#                                         nn.ReLU(True),\n",
    "#                                         nn.Linear(128, 200),\n",
    "                                        nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(data):\n",
    "    noise = torch.randn(data.size()[0], 200) * 0.2\n",
    "    noisy_data = data + noise\n",
    "    return noisy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/20], MSE_loss:124.2227\n",
      "epoch [2/20], MSE_loss:124.6318\n",
      "epoch [3/20], MSE_loss:124.3436\n",
      "epoch [4/20], MSE_loss:125.4180\n",
      "epoch [5/20], MSE_loss:126.5876\n",
      "epoch [6/20], MSE_loss:126.0823\n",
      "epoch [7/20], MSE_loss:126.6260\n",
      "epoch [8/20], MSE_loss:123.6308\n",
      "epoch [9/20], MSE_loss:126.4224\n",
      "epoch [10/20], MSE_loss:123.7523\n",
      "epoch [11/20], MSE_loss:121.6230\n",
      "epoch [12/20], MSE_loss:124.0993\n",
      "epoch [13/20], MSE_loss:124.2260\n",
      "epoch [14/20], MSE_loss:125.7262\n",
      "epoch [15/20], MSE_loss:127.0370\n",
      "epoch [16/20], MSE_loss:125.3770\n",
      "epoch [17/20], MSE_loss:125.5110\n",
      "epoch [18/20], MSE_loss:124.0258\n",
      "epoch [19/20], MSE_loss:122.7660\n",
      "epoch [20/20], MSE_loss:124.7041\n"
     ]
    }
   ],
   "source": [
    "model = autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data, _ in dataloader:\n",
    "        data = data.float()\n",
    "        noisy_data = add_noise(data)\n",
    "        noisy_data = Variable(noisy_data)\n",
    "        data = Variable(data)\n",
    "        \n",
    "        \n",
    "        # ===================forward=====================\n",
    "        output = model(noisy_data)\n",
    "        loss = criterion(output, data)\n",
    "        \n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], MSE_loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, loss.item()))\n",
    "    \n",
    "# torch.save(model.state_dict(), './sim_autoencoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hmmm! its not learning anything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 8.3929e-19, 1.0000e+00, 1.0000e+00, 1.0000e+00, 6.0714e-20,\n",
       "        1.0000e+00, 1.0000e+00, 2.8300e-05, 1.0000e+00, 1.4002e-02, 3.3214e-21,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.4991e-19,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 6.2313e-21, 4.4062e-17, 1.0000e+00, 1.0000e+00,\n",
       "        9.6204e-21, 1.0000e+00, 1.2030e-16, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 4.5482e-21, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 5.7222e-21, 1.0000e+00, 3.2445e-22,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.9982e-17, 1.0000e+00,\n",
       "        1.1110e-09, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 2.8443e-22, 9.9709e-01, 9.9845e-01, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.4602e-20, 1.0000e+00, 6.9374e-01,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 7.0132e-20, 7.8895e-08,\n",
       "        3.5709e-19, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        2.5993e-22, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.5198e-14,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 2.6193e-16, 4.7449e-21, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 2.6770e-21, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 2.5297e-13, 9.3806e-18, 1.0000e+00, 1.0000e+00, 5.6338e-01,\n",
       "        1.0000e+00, 1.0000e+00, 1.6381e-20, 2.5291e-20, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 9.8872e-21, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 3.9580e-19,\n",
       "        1.0000e+00, 2.5239e-20, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 7.0987e-19, 1.0000e+00, 1.0000e+00, 5.8875e-20,\n",
       "        1.0000e+00, 1.0000e+00, 4.8515e-01, 3.1675e-17, 1.0000e+00, 1.4413e-17,\n",
       "        1.0000e+00, 1.0000e+00, 6.8316e-21, 1.0000e+00, 1.1355e-16, 1.0000e+00,\n",
       "        1.4897e-19, 1.0000e+00, 1.2098e-12, 1.0000e+00, 1.0000e+00, 5.3254e-20,\n",
       "        1.0000e+00, 1.1362e-21, 1.0000e+00, 8.9646e-01, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 6.1014e-15, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 3.4885e-20], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
