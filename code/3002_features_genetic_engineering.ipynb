{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to find a mathematical formula that will create a new feature from the ones we have - and therefore give the machine learning algorithms more to work with. In the case of the Santander competition, this is dificult because the features are already well pre-processed. In other words, they are already statistically independent and seem to contain little, if any, redundant information (as if principal component analysis has already been performed). Instead, we can turn to generically engineered formulas to create new features.\n",
    "\n",
    "Specificaly, if we use 199 of the 200 features to create an estimation of the missing feature - this will be a imperfect estimation, because as noted above, the features do not contain redundant information. Nevertheless, this poor estimation can be considered to be a \"new feature\" or a new classification of the current 199 features as being a member of the missing feature target. This can be repeated for every feature, giving us 200 new features to add to the training (and testing) sets.\n",
    "\n",
    "This kernel demonstrates this, and uses \"gplearn\" which extends the scikit-learn machine learning library to perform Genetic Programming (GP) with symbolic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the website https://gplearn.readthedocs.io/en/stable/intro.html ...\n",
    "\n",
    "\"Symbolic regression is a machine learning technique that aims to identify an underlying mathematical expression that best describes a relationship. It begins by building a population of naive random formulas to represent a relationship between known independent variables and their dependent variable targets in order to predict new data. Each successive generation of programs is then evolved from the one that came before it by selecting the fittest individuals from the population to undergo genetic operations.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import time\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# GENETIC ALGORITHM\n",
    "import gplearn\n",
    "from gplearn.genetic import SymbolicTransformer, SymbolicRegressor\n",
    "from gplearn.functions import make_function\n",
    "from gplearn.fitness import make_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../data/train.csv'\n",
    "test_file = '../data/test.csv'\n",
    "\n",
    "df_train = pd.read_csv(train_file)\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X1 = df_train.drop(['ID_code','target'],axis=1)\n",
    "X_test1 = df_test.drop('ID_code',axis=1)\n",
    "\n",
    "# Create a fitness function that is the mean absolute percentage error\n",
    "def _my_fit(y, y_pred, w):\n",
    "    diffs = np.abs(y - y_pred)  \n",
    "    return 100. * np.average(diffs, weights=w)\n",
    "\n",
    "\n",
    "\n",
    "my_fit = make_fitness(_my_fit, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the mathematical functions we will combine together\n",
    "function_set = ['add', 'sub', 'mul', 'div', 'log', \n",
    "                'sqrt', 'log', 'abs', 'neg', 'inv', \n",
    "                'max', 'min', 'sin', 'cos', 'tan' ] \n",
    "\n",
    "# Create the genetic learning regressor\n",
    "gp = SymbolicRegressor(function_set=function_set, metric = my_fit,\n",
    "                       verbose=1, generations = 3, random_state=0, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using NUMPY structures, remove one feature (column of data) at a time from the training set\n",
    "# Use that removed column as the target for the algorithm\n",
    "# Use the genetically engineered formula to create the new feature\n",
    "# Do this for both the training set and the test set\n",
    "\n",
    "X1a = np.array(X1)\n",
    "sam = X1a.shape[0]\n",
    "col = X1a.shape[1]\n",
    "X2a = np.zeros((sam, col))\n",
    "\n",
    "X_test1a = np.array(X_test1)\n",
    "sam_test = X_test1a.shape[0]\n",
    "col_test = X_test1a.shape[1]\n",
    "X_test2a = np.zeros((sam_test, col_test))\n",
    "\n",
    "for i in range(col) :\n",
    "    X = np.delete(X1a,i,1)\n",
    "    y = X1a[:,i]\n",
    "    gp.fit(X, y) \n",
    "    X2a[:,i] = gp.predict(X)\n",
    "    X = np.delete(X_test1a, i, 1)\n",
    "    X_test2a[:,i] = gp.predict(X)\n",
    "    \n",
    "X2 = pd.DataFrame(X2a)\n",
    "X_test2 = pd.DataFrame(X_test2a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new features to the existing 200 features\n",
    "X = pd.concat([X1, X2], axis=1, sort=False) \n",
    "X_test = pd.concat([X_test1, X_test2], axis=1, sort=False)  \n",
    "y = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_leaves': 13,\n",
    "    'min_data_in_leaf': 80,\n",
    "    'objective': 'binary',\n",
    "    'max_depth': -1,\n",
    "    'learning_rate': 0.01,\n",
    "    'boosting': 'gbdt',\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.33,\n",
    "    'feature_fraction': 0.05,\n",
    "    'metric':'auc',\n",
    "    'verbosity': 1, \n",
    "    'min_sum_hessian_in_leaf': 10.0,\n",
    "    'num_threads': 12,\n",
    "    'tree_learner': 'serial',\n",
    "    'boost_from_average':'false'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "\n",
    "prediction = np.zeros(len(X_test))\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):\n",
    "    print('Fold', fold_n, 'started at', time.ctime())\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "        \n",
    "    model = lgb.train(params,train_data,num_boost_round=20000,\n",
    "                    valid_sets = [train_data, valid_data],verbose_eval=300,early_stopping_rounds = 200)\n",
    "             \n",
    "    y_pred_valid += model.predict(X_valid)/n_fold\n",
    "    prediction += model.predict(X_test, num_iteration=model.best_iteration)/n_fold \n",
    "    \n",
    "val_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "print('Validation AUC: ', val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\n",
    "sub[\"target\"] = prediction\n",
    "sub.to_csv(\"../results/gplearn_xgb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
